+++
date = '2025-02-04T00:00:00-08:00'
draft = false
title = '15 Questions'
+++

1. What do truly long-context models look like? 
    I want to give the model all my journals, notes, pictures, previous work, etc. so that it can make connections and tailor responses for me. 
    I imagine this to be In-between context stuffing and fine-tuning. Every ~day, the model takes all the conversations from that day and decides which to use to update its weights. In the future, will everyone have custom models?
    Predictive processing?
2. What will human-AI collaboration look like in the future? 
3. How much software will humans be writing in three years? 
What are the comparative advantages of humans? 
4. Is "We don't need to find the most general, all-modality, solution. We just need to get something good enough to automate research. That's the goal. After that, there's a clear path and we're just on high-level steering." wrong? 
5. Has someone created a gym environment that is a computer simulation? Actions are anything someone can do on a computer. After each episode, unit tests are run to determine reward. Why are we using screenshots?
6. How much does o1-style reasoning RL transfer to performing long-horizon tasks for computer use?
7. I don't get how we're passing the synthetic data wall. Yes, you can use o3 outputs to fine-tune 4o and get a really good o3-mini, but can you use oN outputs to get oN+1?
8. Can you get two models to communicate through residual streams and not text? Or CoT in the latent space instead of writing everything out? Is this desirable? How do you get training data for this? 
A quick perplexity search [gets](https://transformer-circuits.pub/2023/privileged-basis/index.html) [me](https://arxiv.org/html/2406.03230v2) [these](https://www.alignmentforum.org/posts/X26ksz4p3wSyycKNB/gears-level-mental-models-of-transformer-interpretability) [links](https://www.reddit.com/r/LocalLLaMA/comments/1gxxqs9/why_should_thoughts_be_word_tokens_in_o1_style/).
9. We have text-to-text, text-to-image, text-to-video. What is the SOTA for text-to-action tokens in robots? There must be a way to leverage the understanding of the world language models have to robotics. How?
10. How much do traders use ML? It seems like a ripe field for it. Lots of money, data, smart peopleâ€¦ Everything is probably private.
11. Why is Moravec's paradox true?
12. How is Adam still the best optimizer after 10 years? `	
13. How do lightweight code-generation models like Cursor's work?
14. What is going on in interpretability these days?
15. Why are all the benchmarks in math and coding competitions? What happened to physics?










