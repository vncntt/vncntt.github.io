+++
title = 'Truly Long Context'
draft = true 
+++



[Inspiration](https://x.com/gallabytes/status/1881124186669879401)

2 million tokens is not enough. Or rather it's just a 


In the thread, the author talks about "softening the train time versus inference time disctintion."
We as humans operate this way. 
Our inference and training are done together in some sense. 
A theory for this is predictive processing. 

I want a model that knows everything about me: a best friend, assistant, mentor, etc.


What does this look like?
I 

Right now, each conversation is fresh and new. 
I hate this. 
I want to model to know what I was thinking about and working on yesterday as well as 2 months ago.
Something in between training on all my previous chats and just stuffing it in the context.


