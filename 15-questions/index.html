<!doctype html><html class="not-ready lg:text-base" style=--bg:#faf8f1 lang=en-us dir=ltr><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>15 Questions - Vincent Cheng</title><meta name=theme-color><meta name=description content="
What do truly long-context models look like?
I want to give the model all my journals, notes, pictures, previous work, etc. so that it can make connections and tailor responses for me.
I imagine this to be In-between context stuffing and fine-tuning. Every ~day, the model takes all the conversations from that day and decides which to use to update its weights. In the future, will everyone have custom models?
Predictive processing?
What will human-AI collaboration look like in the future?
How much software will humans be writing in three years?
What are the comparative advantages of humans?
Is &ldquo;We don&rsquo;t need to find the most general, all-modality, solution. We just need to get something good enough to automate research. That&rsquo;s the goal. After that, there&rsquo;s a clear path and we&rsquo;re just on high-level steering.&rdquo; wrong?
Has someone created a gym environment that is a computer simulation? Actions are anything someone can do on a computer. After each episode, unit tests are run to determine reward. Why are we using screenshots?
How much does o1-style reasoning RL transfer to performing long-horizon tasks for computer use?
I don&rsquo;t get how we&rsquo;re passing the synthetic data wall. Yes, you can use o3 outputs to fine-tune 4o and get a really good o3-mini, but can you use oN outputs to get oN+1?
Can you get two models to communicate through residual streams and not text? Or CoT in the latent space instead of writing everything out? Is this desirable? How do you get training data for this?
A quick perplexity search gets me these links.
We have text-to-text, text-to-image, text-to-video. What is the SOTA for text-to-action tokens in robots? There must be a way to leverage the understanding of the world language models have to robotics. How?
How much do traders use ML? It seems like a ripe field for it. Lots of money, data, smart people… Everything is probably private.
Why is Moravec&rsquo;s paradox true?
How is Adam still the best optimizer after 10 years? `
How do lightweight code-generation models like Cursor&rsquo;s work?
What is going on in interpretability these days?
Why are all the benchmarks in math and coding competitions? What happened to physics?
"><meta name=author content="Vincent Cheng"><link rel="preload stylesheet" as=style href=https://vvvincent.me/main.min.css><link rel=preload as=image href=https://vvvincent.me/theme.png><link rel=preload as=image href=https://vvvincent.me/twitter.svg><link rel=preload as=image href=https://vvvincent.me/github.svg><script defer src=https://vvvincent.me/highlight.min.js onload=hljs.initHighlightingOnLoad()></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",()=>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1}))</script><link rel=icon href=https://vvvincent.me/favicon.ico><link rel=apple-touch-icon href=https://vvvincent.me/apple-touch-icon.png><meta name=generator content="Hugo 0.147.9"><meta itemprop=name content="15 Questions"><meta itemprop=description content="What do truly long-context models look like? I want to give the model all my journals, notes, pictures, previous work, etc. so that it can make connections and tailor responses for me. I imagine this to be In-between context stuffing and fine-tuning. Every ~day, the model takes all the conversations from that day and decides which to use to update its weights. In the future, will everyone have custom models? Predictive processing? What will human-AI collaboration look like in the future? How much software will humans be writing in three years? What are the comparative advantages of humans? Is “We don’t need to find the most general, all-modality, solution. We just need to get something good enough to automate research. That’s the goal. After that, there’s a clear path and we’re just on high-level steering.” wrong? Has someone created a gym environment that is a computer simulation? Actions are anything someone can do on a computer. After each episode, unit tests are run to determine reward. Why are we using screenshots? How much does o1-style reasoning RL transfer to performing long-horizon tasks for computer use? I don’t get how we’re passing the synthetic data wall. Yes, you can use o3 outputs to fine-tune 4o and get a really good o3-mini, but can you use oN outputs to get oN+1? Can you get two models to communicate through residual streams and not text? Or CoT in the latent space instead of writing everything out? Is this desirable? How do you get training data for this? A quick perplexity search gets me these links. We have text-to-text, text-to-image, text-to-video. What is the SOTA for text-to-action tokens in robots? There must be a way to leverage the understanding of the world language models have to robotics. How? How much do traders use ML? It seems like a ripe field for it. Lots of money, data, smart people… Everything is probably private. Why is Moravec’s paradox true? How is Adam still the best optimizer after 10 years? ` How do lightweight code-generation models like Cursor’s work? What is going on in interpretability these days? Why are all the benchmarks in math and coding competitions? What happened to physics?"><meta itemprop=datePublished content="2025-02-04T00:00:00-08:00"><meta itemprop=dateModified content="2025-02-04T00:00:00-08:00"><meta itemprop=wordCount content="364"><meta itemprop=image content="https://vvvincent.me/photo_new.jpg"><meta property="og:url" content="https://vvvincent.me/15-questions/"><meta property="og:site_name" content="Vincent Cheng"><meta property="og:title" content="15 Questions"><meta property="og:description" content="What do truly long-context models look like? I want to give the model all my journals, notes, pictures, previous work, etc. so that it can make connections and tailor responses for me. I imagine this to be In-between context stuffing and fine-tuning. Every ~day, the model takes all the conversations from that day and decides which to use to update its weights. In the future, will everyone have custom models? Predictive processing? What will human-AI collaboration look like in the future? How much software will humans be writing in three years? What are the comparative advantages of humans? Is “We don’t need to find the most general, all-modality, solution. We just need to get something good enough to automate research. That’s the goal. After that, there’s a clear path and we’re just on high-level steering.” wrong? Has someone created a gym environment that is a computer simulation? Actions are anything someone can do on a computer. After each episode, unit tests are run to determine reward. Why are we using screenshots? How much does o1-style reasoning RL transfer to performing long-horizon tasks for computer use? I don’t get how we’re passing the synthetic data wall. Yes, you can use o3 outputs to fine-tune 4o and get a really good o3-mini, but can you use oN outputs to get oN+1? Can you get two models to communicate through residual streams and not text? Or CoT in the latent space instead of writing everything out? Is this desirable? How do you get training data for this? A quick perplexity search gets me these links. We have text-to-text, text-to-image, text-to-video. What is the SOTA for text-to-action tokens in robots? There must be a way to leverage the understanding of the world language models have to robotics. How? How much do traders use ML? It seems like a ripe field for it. Lots of money, data, smart people… Everything is probably private. Why is Moravec’s paradox true? How is Adam still the best optimizer after 10 years? ` How do lightweight code-generation models like Cursor’s work? What is going on in interpretability these days? Why are all the benchmarks in math and coding competitions? What happened to physics?"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-02-04T00:00:00-08:00"><meta property="article:modified_time" content="2025-02-04T00:00:00-08:00"><meta property="og:image" content="https://vvvincent.me/photo_new.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://vvvincent.me/photo_new.jpg"><meta name=twitter:title content="15 Questions"><meta name=twitter:description content="What do truly long-context models look like? I want to give the model all my journals, notes, pictures, previous work, etc. so that it can make connections and tailor responses for me. I imagine this to be In-between context stuffing and fine-tuning. Every ~day, the model takes all the conversations from that day and decides which to use to update its weights. In the future, will everyone have custom models? Predictive processing? What will human-AI collaboration look like in the future? How much software will humans be writing in three years? What are the comparative advantages of humans? Is “We don’t need to find the most general, all-modality, solution. We just need to get something good enough to automate research. That’s the goal. After that, there’s a clear path and we’re just on high-level steering.” wrong? Has someone created a gym environment that is a computer simulation? Actions are anything someone can do on a computer. After each episode, unit tests are run to determine reward. Why are we using screenshots? How much does o1-style reasoning RL transfer to performing long-horizon tasks for computer use? I don’t get how we’re passing the synthetic data wall. Yes, you can use o3 outputs to fine-tune 4o and get a really good o3-mini, but can you use oN outputs to get oN+1? Can you get two models to communicate through residual streams and not text? Or CoT in the latent space instead of writing everything out? Is this desirable? How do you get training data for this? A quick perplexity search gets me these links. We have text-to-text, text-to-image, text-to-video. What is the SOTA for text-to-action tokens in robots? There must be a way to leverage the understanding of the world language models have to robotics. How? How much do traders use ML? It seems like a ripe field for it. Lots of money, data, smart people… Everything is probably private. Why is Moravec’s paradox true? How is Adam still the best optimizer after 10 years? ` How do lightweight code-generation models like Cursor’s work? What is going on in interpretability these days? Why are all the benchmarks in math and coding competitions? What happened to physics?"><link rel=canonical href=https://vvvincent.me/15-questions/></head><body class="text-black duration-200 ease-out dark:text-white"><header class="mx-auto flex h-[4.5rem] max-w-[--w] px-8 lg:justify-center"><div class="relative z-50 ltr:mr-auto rtl:ml-auto flex items-center"><a class="-translate-y-[1px] text-2xl font-medium" href=https://vvvincent.me/>Vincent Cheng</a><div class="btn-dark text-[0] ltr:ml-4 rtl:mr-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]" role=button aria-label=Dark></div></div><div class="btn-menu relative z-50 ltr:-mr-8 rtl:-ml-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden" role=button aria-label=Menu></div><script>const htmlClass=document.documentElement.classList;setTimeout(()=>{htmlClass.remove("not-ready")},10);const btnMenu=document.querySelector(".btn-menu");btnMenu.addEventListener("click",()=>{htmlClass.toggle("open")});const metaTheme=document.querySelector('meta[name="theme-color"]'),lightBg="#faf8f1".replace(/"/g,""),setDark=e=>{metaTheme.setAttribute("content",e?"#000":lightBg),htmlClass[e?"add":"remove"]("dark"),localStorage.setItem("dark",e)},darkScheme=window.matchMedia("(prefers-color-scheme: dark)");if(htmlClass.contains("dark"))setDark(!0);else{const e=localStorage.getItem("dark");setDark(e?e==="true":darkScheme.matches)}darkScheme.addEventListener("change",e=>{setDark(e.matches)});const btnDark=document.querySelector(".btn-dark");btnDark.addEventListener("click",()=>{setDark(localStorage.getItem("dark")!=="true")})</script><div class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"><nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-10 rtl:space-x-reverse"><a class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal" href=/about/>About</a>
<a class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal" href=/now/>Now</a></nav><nav class="mt-12 flex justify-center space-x-10 rtl:space-x-reverse dark:invert ltr:lg:ml-14 rtl:lg:mr-14 lg:mt-0 lg:items-center"><a class="h-7 w-7 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" style=--url:url(./twitter.svg) href=https://twitter.com/vvvincent_c target=_blank rel=me>twitter
</a><a class="h-7 w-7 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" style=--url:url(./github.svg) href=https://github.com/vncntt target=_blank rel=me>github</a></nav></div></header><main class="prose prose-neutral relative mx-auto min-h-[calc(100vh-9rem)] max-w-[--w] px-8 pb-16 pt-14 dark:prose-invert"><article><header class=mb-14><h1 class="!my-0 pb-2.5">15 Questions</h1><div class="text-xs antialiased opacity-60"><time>Feb 4, 2025</time></div></header><section><ol><li>What do truly long-context models look like?
I want to give the model all my journals, notes, pictures, previous work, etc. so that it can make connections and tailor responses for me.
I imagine this to be In-between context stuffing and fine-tuning. Every ~day, the model takes all the conversations from that day and decides which to use to update its weights. In the future, will everyone have custom models?
Predictive processing?</li><li>What will human-AI collaboration look like in the future?</li><li>How much software will humans be writing in three years?
What are the comparative advantages of humans?</li><li>Is &ldquo;We don&rsquo;t need to find the most general, all-modality, solution. We just need to get something good enough to automate research. That&rsquo;s the goal. After that, there&rsquo;s a clear path and we&rsquo;re just on high-level steering.&rdquo; wrong?</li><li>Has someone created a gym environment that is a computer simulation? Actions are anything someone can do on a computer. After each episode, unit tests are run to determine reward. Why are we using screenshots?</li><li>How much does o1-style reasoning RL transfer to performing long-horizon tasks for computer use?</li><li>I don&rsquo;t get how we&rsquo;re passing the synthetic data wall. Yes, you can use o3 outputs to fine-tune 4o and get a really good o3-mini, but can you use oN outputs to get oN+1?</li><li>Can you get two models to communicate through residual streams and not text? Or CoT in the latent space instead of writing everything out? Is this desirable? How do you get training data for this?
A quick perplexity search <a href=https://transformer-circuits.pub/2023/privileged-basis/index.html>gets</a> <a href=https://arxiv.org/html/2406.03230v2>me</a> <a href=https://www.alignmentforum.org/posts/X26ksz4p3wSyycKNB/gears-level-mental-models-of-transformer-interpretability>these</a> <a href=https://www.reddit.com/r/LocalLLaMA/comments/1gxxqs9/why_should_thoughts_be_word_tokens_in_o1_style/>links</a>.</li><li>We have text-to-text, text-to-image, text-to-video. What is the SOTA for text-to-action tokens in robots? There must be a way to leverage the understanding of the world language models have to robotics. How?</li><li>How much do traders use ML? It seems like a ripe field for it. Lots of money, data, smart people… Everything is probably private.</li><li>Why is Moravec&rsquo;s paradox true?</li><li>How is Adam still the best optimizer after 10 years? `</li><li>How do lightweight code-generation models like Cursor&rsquo;s work?</li><li>What is going on in interpretability these days?</li><li>Why are all the benchmarks in math and coding competitions? What happened to physics?</li></ol></section><nav class="mt-24 flex overflow-hidden rounded-xl bg-black/[3%] text-lg !leading-[1.2] *:flex *:w-1/2 *:items-center *:p-5 *:font-medium *:no-underline dark:bg-white/[8%] [&>*:hover]:bg-black/[2%] dark:[&>*:hover]:bg-white/[3%]"><a class="ltr:pr-3 rtl:pl-3" href=https://vvvincent.me/metal-pins-simulation/><span class="ltr:mr-1.5 rtl:ml-1.5">←</span><span>Metal Pins Simulation</span></a>
<a class="ltr:ml-auto rtl:mr-auto justify-end pl-3" href=https://vvvincent.me/ideas/><span>Ideas</span><span class="ltr:ml-1.5 rtl:mr-1.5">→</span></a></nav></article></main><footer class="mx-auto flex h-[4.5rem] max-w-[--w] items-center px-8 text-xs uppercase tracking-wider opacity-60"><div class=mr-auto>&copy; 2025
<a class=link href=https://vvvincent.me/>Vincent Cheng</a></div><a class="link mx-6" href=https://gohugo.io/ rel=noopener target=_blank>powered by hugo️️</a>️
<a class=link href=https://github.com/nanxiaobei/hugo-paper rel=noopener target=_blank>hugo-paper</a></footer></body></html>